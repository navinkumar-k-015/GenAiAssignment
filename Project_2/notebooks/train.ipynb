{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3d7efe-b2de-46f0-8dcb-7668308b965e",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae9de6e-6230-4d86-b707-6bc753056f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd34ac8bf41a4917b22c3be2dc7d4b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21861 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9674b2804094a36bd34a51c6c521443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee26eddb90b64b7fa52b6c341a2702da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4101' max='4101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4101/4101 1:31:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.187570</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.962613</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.962383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.153029</td>\n",
       "      <td>0.964874</td>\n",
       "      <td>0.965489</td>\n",
       "      <td>0.964874</td>\n",
       "      <td>0.964961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.142295</td>\n",
       "      <td>0.972192</td>\n",
       "      <td>0.972307</td>\n",
       "      <td>0.972192</td>\n",
       "      <td>0.972212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 01:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9612147822905233\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "\n",
    "# ========== 1️⃣ Load Train, Validation, and Test Data ==========\n",
    "train_df = pd.read_csv(\"train_set_1.csv\")\n",
    "val_df = pd.read_csv(\"val_set_1.csv\")\n",
    "test_df = pd.read_csv(\"test_set_1.csv\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "\n",
    "# Get the class distribution\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"label\"] = label_encoder.fit_transform(train_df[\"label\"])\n",
    "val_df[\"label\"] = label_encoder.transform(val_df[\"label\"])\n",
    "test_df[\"label\"] = label_encoder.transform(test_df[\"label\"])\n",
    "\n",
    "class_counts = Counter(train_df['label'])\n",
    "classes = np.unique(train_df['label'])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_df['label'])\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "# ========== 2️⃣ Tokenization ==========\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights_tensor = torch.tensor(list(class_weight_dict.values()), dtype=torch.float, device=device)\n",
    "\n",
    "def encode_texts(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Convert Pandas DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Apply Tokenization\n",
    "train_dataset = train_dataset.map(encode_texts, batched=True)\n",
    "val_dataset = val_dataset.map(encode_texts, batched=True)\n",
    "test_dataset = test_dataset.map(encode_texts, batched=True)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# ========== 3️⃣ Load Pre-trained BERT Model ==========\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# ========== 4️⃣ Define Training Arguments ==========\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  # Save class weights\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Forward pass\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        logits = logits.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Use CrossEntropyLoss with class weights if labels are provided\n",
    "        if self.class_weights is not None and labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# ========== 5️⃣ Define Metrics Function ==========\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ========== 6️⃣ Train Model using Trainer ==========\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========== 7️⃣ Save the Trained Model ==========\n",
    "trainer.save_model(\"./bert_text_classifier\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# ========== 8️⃣ Evaluate on Test Set ==========\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Accuracy:\", test_results[\"eval_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fb4c5-2365-4149-87f9-baef20002c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    before = text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "    text = \" \".join(filtered_text)\n",
    "    # print_change(before, text, \"Remove Stopwords\")\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    before = text\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    # print_change(before, lemmatized_text, \"Lemmatization\")\n",
    "    return lemmatized_text\n",
    "\n",
    "def lowercase_text(text):\n",
    "    before = text\n",
    "    text = text.lower()\n",
    "    # print_change(before, text, \"Lowercasing\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    before = text\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    text = text + '.'  # Append full stop\n",
    "    # print_change(before, text, \"Remove Whitespaces and Add Full Stop\")\n",
    "    return text\n",
    "\n",
    "# 7. Removing URLs\n",
    "def remove_urls(text):\n",
    "    before = text\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # print_change(before, text, \"Remove URLs\")\n",
    "    return text\n",
    "\n",
    "# 8. Replace Ampersand (&) with 'and' and Similar Substitutions\n",
    "def replace_ampersand(text):\n",
    "    before = text\n",
    "    substitutions = {\n",
    "        \"&\": \"and\",\n",
    "        \"%\": \"percent\",\n",
    "        \"$\": \"dollar\",\n",
    "        \"₹\": 'rs.',\n",
    "        \"@\": \"\",\n",
    "        \"*\": \"x\",\n",
    "        \"#\":'',\n",
    "        '\"': ' ',       \n",
    "        \"'s\": ' ',      \n",
    "        \"'\": '',       \n",
    "        \"_\": ' ',       \n",
    "        \"=\": ' ',       \n",
    "        \"|\": ' ',\n",
    "    }\n",
    "    \n",
    "    for old, new in substitutions.items():\n",
    "        text = text.replace(old, new)\n",
    "    text = re.sub(r'[©®™~^<>\\\\/`\\[\\]\\(\\)\\{\\}]', ' ', text)\n",
    "    # print_change(before, text, \"Replace Ampersand (&) and Similar Substitutions\")\n",
    "    return text\n",
    "\n",
    "# 9. Replace Model Numbers or Part Numbers\n",
    "def replace_model_numbers(text):\n",
    "    before = text\n",
    "    # Regex to match common model/part number patterns\n",
    "    # Match sequences like 'ABC123', '123-XYZ', 'ABC-1234', etc.\n",
    "    model_number_pattern = r'(?<!\\s)([A-Za-z0-9]+(?:[-_\\][A-Za-z0-9]+)+)(?!\\s)'\n",
    "    \n",
    "    # Only replace model numbers with <MODEL>\n",
    "    text = re.sub(model_number_pattern, lambda match: '<MODEL>' if any(c.isdigit() for c in match.group(0)) else match.group(0), text)\n",
    "    \n",
    "    # print_change(before, text, \"Model\")\n",
    "    return text\n",
    "\n",
    "def remove_repeated_phrases(text):\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Keep track of seen phrases\n",
    "    seen_phrases = set()\n",
    "\n",
    "    # List to store words that are not repeated\n",
    "    result = []\n",
    "\n",
    "    # Iterate through words and construct phrases\n",
    "    for i, word in enumerate(words):\n",
    "        # Construct potential phrase by joining words\n",
    "        phrase = ' '.join(words[i:i+1])  # Adjust the range for longer phrases if needed\n",
    "\n",
    "        # Check if phrase is seen\n",
    "        if phrase not in seen_phrases:\n",
    "            result.append(word)\n",
    "            seen_phrases.add(phrase)\n",
    "\n",
    "    # Join the result list into a string\n",
    "    return ' '.join(result)\n",
    "# Combining All Preprocessing Steps\n",
    "def preprocess_text(text):\n",
    "    # print(f\"Original Text: {text[:100]}...\")  # Show original text (first 100 characters)\n",
    "    \n",
    "    # Call each function and apply transformations\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = replace_ampersand(text)\n",
    "    text = remove_whitespaces(text)\n",
    "    text = remove_urls(text)\n",
    "    text = replace_model_numbers(text)\n",
    "    text = remove_repeated_phrases(text)\n",
    "    \n",
    "    # print(f\"Processed Text: {text[:100]}...\")  # Show processed text (first 100 characters)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923998da-8a7c-4fcb-b9f6-0e156c2e4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from api.model.utils.clean_text import preprocess_text\n",
    "import torch.nn.functional as F\n",
    "def predict_class(text):\n",
    "    # Tokenize input text\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move tensors to the same device as model (GPU/CPU)\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Perform inference (no need to compute gradients)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class (index of the highest logit)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    class_scores = logits.squeeze().cpu().numpy()\n",
    "    probabilities = F.softmax(torch.tensor(class_scores), dim=0).numpy()\n",
    "\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "# Example usage\n",
    "input_text = \"I love computers with mac insalled\"\n",
    "\n",
    "predicted_class, probabilities = predict_class(input_text)\n",
    "\n",
    "# Mapping predicted class to your labels (assuming you have 4 classes)\n",
    "labels = [\"Household\", \"Books\", \"Electronics\", \"Clothing & Accessories\"]\n",
    "\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "probabilities_with_labels = list(zip(labels, probabilities))\n",
    "\n",
    "# Sorting probabilities with labels in descending order to see the top classes\n",
    "probabilities_with_labels = sorted(probabilities_with_labels, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Predicted class: {predicted_label}\")\n",
    "print(f\"Class probabilities with labels: {probabilities_with_labels}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
